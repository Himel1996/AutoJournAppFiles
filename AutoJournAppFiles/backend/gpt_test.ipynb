{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from config import Config\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from topic_modeling.topic_modeling import TopicModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = Config.OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY_Ahmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "    What is the capital of Australia?\n",
    "    \"\"\"\n",
    "response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an expert that provides multiple perspectives on statements.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Australia is Canberra. This city was selected as a compromise between Sydney and Melbourne, two major cities that were both vying to be the capital. Located in the Australian Capital Territory, Canberra is the political center of the country, housing important government institutions such as the Parliament House and the High Court of Australia.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptTopicModeling(TopicModeling):\n",
    "    \"\"\"\n",
    "    Topic Modeling using the Mistral model via LangChain's OllamaLLM.\n",
    "    Dynamically generates topics based on input data and a user-defined prompt.\n",
    "    \"\"\"\n",
    "    def __init__(self, api_key: str=api_key, num_topics: int = 5, model_name: str = \"gpt-4o\"):\n",
    "        self.num_topics = num_topics\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Clean the input text by removing links, tags, and emojis.\"\"\"\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)  # Remove links.\n",
    "        text = re.sub(r\"@\\S+\", \"\", text)  # Remove tags.\n",
    "        text = re.sub(r\"\\w+:\\s?\", \"\", text)  # Remove author at the start.\n",
    "        text = self.__remove_emojis(text)\n",
    "\n",
    "        text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions\n",
    "        text = re.sub(r\"#\\w+\", \"\", text)  # Remove hashtags\n",
    "        text = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", text)  # Remove non-alphanumeric characters\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "        return text\n",
    "\n",
    "    def __remove_emojis(self, text):\n",
    "        \"\"\"Remove emojis from text.\"\"\"\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # Emoticons.\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs.\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols.\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # Flags.\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # Function to clean and extract JSON from a response\n",
    "    def extract_json(self, response_text):\n",
    "        try:\n",
    "        # Use a regex to find the JSON object within the response\n",
    "            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group(0)\n",
    "                return json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Error decoding JSON:\", e)\n",
    "        return None\n",
    "    def clean_response(self, response):\n",
    "        response = response.replace(\"\\n\", \"\")\n",
    "        cleaned_str = re.sub(r\"```json|```\", \"\", response)  # Remove backticks and \"```json\"\n",
    "        cleaned_str = re.sub(r\"\\\\\", \"\", cleaned_str)  # Remove backslashes\n",
    "        cleaned_str = re.sub(r\"\\'\", '\"', cleaned_str)  # Replace escaped single quotes (\\' -> \")\n",
    "        cleaned_str = re.sub(r'\"s ', ' ', cleaned_str)  # Replace \"s with space\n",
    "        cleaned_str = re.sub(r\"\\s+\", \" \", cleaned_str)  # Replace multiple spaces with single space\n",
    "        return cleaned_str.strip()\n",
    "        \n",
    "    def get_topics(self, doc):\n",
    "        \"\"\"\n",
    "        Generate topics dynamically using the Mistral model.\n",
    "        Returns topic probabilities, keywords, topic names, and topic details.\n",
    "        \"\"\"\n",
    "        preprocessed_doc = self.preprocess(doc)\n",
    "        prompt = f\"\"\"\n",
    "        Analyze the following text and provide at least {self.num_topics} most suitable topics for it, along with their percentages and the relevant 15 keywords.\n",
    "        Text:\n",
    "        {preprocessed_doc}\n",
    "        Format the output as a JSON object: {{\"topics\": [{{\"name\": \"Topic1\", \"percentage\": 25.0, \"keywords\": ['keyword1','keyword2',..]}}, ...]}}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful NLP assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            response_text = response.choices[0].message.content.strip()\n",
    "            cleaned_response = self.clean_response(response_text)\n",
    "            data = self.extract_json(cleaned_response)\n",
    "\n",
    "            if not data:\n",
    "                raise ValueError(\"No valid JSON extracted from model response.\")\n",
    "\n",
    "            topics_and_keywords = data[\"topics\"]\n",
    "            topics = [topic[\"name\"] for topic in topics_and_keywords]\n",
    "            probs = {topic[\"name\"]: topic[\"percentage\"] for topic in topics_and_keywords}\n",
    "            keywords = {topic[\"name\"]: topic[\"keywords\"] for topic in topics_and_keywords}\n",
    "\n",
    "            return probs, keywords, topics, topics_and_keywords\n",
    "        except Exception as e:\n",
    "            print(f\"Error in get_topics: {e}\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "    def get_topics_test(self, doc):\n",
    "        \"\"\"\n",
    "        Mock version of the get_topics method for testing without GPU.\n",
    "        Generates hardcoded topics and keywords as a response.\n",
    "        \n",
    "        Parameters:\n",
    "        - doc: Input document (not used in this test version).\n",
    "        \n",
    "        Returns:\n",
    "        - probs: Hardcoded probabilities for each topic.\n",
    "        - keywords: Hardcoded keywords for each topic.\n",
    "        - topics: List of topic names.\n",
    "        \"\"\"\n",
    "        # Example hardcoded response\n",
    "        probs = {\n",
    "            \"Topic 1\": 30.0,\n",
    "            \"Topic 2\": 25.0,\n",
    "            \"Topic 3\": 20.0,\n",
    "            \"Topic 4\": 15.0,\n",
    "            \"Topic 5\": 10.0\n",
    "        }\n",
    "        \n",
    "        keywords = {\n",
    "            \"Topic 1\": [\"keyword1\", \"keyword2\", \"keyword3\", \"keyword4\", \"keyword5\"],\n",
    "            \"Topic 2\": [\"keyword6\", \"keyword7\", \"keyword8\", \"keyword9\", \"keyword10\"],\n",
    "            \"Topic 3\": [\"keyword11\", \"keyword12\", \"keyword13\", \"keyword14\", \"keyword15\"],\n",
    "            \"Topic 4\": [\"keyword16\", \"keyword17\", \"keyword18\", \"keyword19\", \"keyword20\"],\n",
    "            \"Topic 5\": [\"keyword21\", \"keyword22\", \"keyword23\", \"keyword24\", \"keyword25\"]\n",
    "        }\n",
    "        \n",
    "        topics = list(probs.keys())\n",
    "\n",
    "        topics_and_keywords = [\n",
    "            {\"name\": topic, \"percentage\": probs[topic], \"keywords\": keywords[topic]} for topic in topics\n",
    "        ]\n",
    "        \n",
    "        return probs, keywords, topics, topics_and_keywords\n",
    "\n",
    "    def check_topic_count(self, num_topics):\n",
    "        \"\"\"Ensure the number of topics matches the specified count.\"\"\"\n",
    "        if self.num_topics != num_topics:\n",
    "            self.num_topics = num_topics\n",
    "    \n",
    "\n",
    "    ##Summarization Zone:\n",
    "   \n",
    "    def summarize_with_hint(self, text, topics_and_keywords):\n",
    "        \"\"\"Generate summaries for each topic using LLM.\"\"\"\n",
    "        summaries = []\n",
    "        for topic in topics_and_keywords:\n",
    "            topic_name = topic[\"name\"]\n",
    "            keywords = ', '.join(topic[\"keywords\"][:5])  # Use top 5 keywords\n",
    "            prompt = f\"\"\"\n",
    "            Summarize the following text in 2-3 sentences, focusing on the provided topic and keywords.\n",
    "            Hint: Topic - {topic_name}; Keywords - {keywords}.\n",
    "\n",
    "            Ensure the summary is concise and captures the main points of the text without adding many details.\n",
    "            If the text is very short, condense the summary into a single, clear, and well-structured sentence.\n",
    "            Avoid repeating keywords unnecessarily or adding unrelated details.\n",
    "\n",
    "            Text: {text}\n",
    "            \"\"\"\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model_name,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a summarization assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                summaries.append({\n",
    "                    \"topic\": topic_name,\n",
    "                    \"summary\": response.choices[0].message.content.strip()\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error summarizing for topic {topic_name}: {e}\")\n",
    "        return summaries\n",
    "    \n",
    "    def summarize_with_hint_test(self, text, topics_and_keywords):\n",
    "        \"\"\"Mock version of summarize_with_hint for testing purposes.\"\"\"\n",
    "        summaries = []\n",
    "        for topic in topics_and_keywords:\n",
    "            topic_name = topic[\"name\"]\n",
    "            keywords = ', '.join(topic[\"keywords\"][:5])  # Use top 5 keywords\n",
    "            mock_summary = f\"This is a mock summary for the topic '{topic_name}' with focus on keywords: {keywords}.\"\n",
    "            summaries.append({\"topic\": topic_name, \"summary\": mock_summary})\n",
    "        return summaries\n",
    "    \n",
    "    def generate_news_article(self, topic, summary, keywords, style):\n",
    "        \"\"\"Generate a long news article based on topic, summary, keywords, and style.\"\"\"\n",
    "        style_prompt = {\n",
    "            \"formal\": \"Use a professional and objective tone, suitable for a reputable news outlet.\",\n",
    "            \"academic\": \"Write in an academic style, using analytical and precise language.\",\n",
    "            \"gen_z\": \"Use a Gen Z tone, be witty, include pop culture references, and keep it conversational.\",\n",
    "            \"narrative\": \"Write in a storytelling style, with vivid descriptions and engaging narrative techniques.\",\n",
    "            \"persuasive\": \"Write persuasively, using emotional and motivational language.\",\n",
    "            \"satirical\": \"Write in a satirical tone, with humor and irony to critique the subject.\",\n",
    "            \"conversational\": \"Use a friendly and informal conversational tone.\",\n",
    "            \"poetic\": \"Write in a poetic style, with metaphorical and rhythmic language.\",\n",
    "            \"investigative\": \"Write in an investigative tone, presenting facts systematically and focusing on analysis.\"\n",
    "        }\n",
    "\n",
    "        tone_instructions = style_prompt[style]\n",
    "\n",
    "        prompt = (\n",
    "            f\"You are a professional journalist writing for a major news outlet. Your goal is to craft a compelling and detailed news article.\\n\\n\"\n",
    "            f\"**Topic**: {topic}\\n\\n\"\n",
    "            f\"**Summary**: {summary}\\n\\n\"\n",
    "            f\"**Key Points and Keywords**:\\n- \" + \"\\n- \".join(keywords) + \"\\n\\n\"\n",
    "            f\"**Style**: {tone_instructions}\\n\\n\"\n",
    "            f\"**Requirements**:\\n\"\n",
    "            f\"1. Write a long, engaging news article (at least 800 words).\\n\"\n",
    "            f\"2. Include an attention-grabbing headline at the beginning.\\n\"\n",
    "            f\"3. Expand upon the provided summary using the listed keywords. Use them naturally throughout the article.\\n\"\n",
    "            f\"4. Include historical context, background, or analysis where relevant.\\n\"\n",
    "            f\"5. Use the specified style and tone throughout the article.\\n\\n\"\n",
    "            f\"Start your response with the headline, followed by the full article.\"\n",
    "        )\n",
    "\n",
    "        print(f\"Generating article for topic: {topic} in {style} style\")\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a creative journalist.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.8\n",
    "            )\n",
    "            print(f\"Article generated for topic: {topic}\")\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating article for topic {topic}: {e}\")\n",
    "            return \"\"\n",
    "       \n",
    "    \n",
    "    def generate_news_article_test(self, topic, summary, keywords, style):\n",
    "        \"\"\"Mock version of generate_news_article for testing purposes.\"\"\"\n",
    "        style_descriptions = {\n",
    "            \"formal\": \"a professional and objective tone, suitable for a reputable news outlet.\",\n",
    "            \"academic\": \"an academic style, using analytical and precise language.\",\n",
    "            \"gen_z\": \"a Gen Z tone, witty and conversational with pop culture references.\",\n",
    "            \"narrative\": \"a storytelling style with vivid descriptions.\",\n",
    "            \"persuasive\": \"a persuasive tone, using emotional and motivational language.\",\n",
    "            \"satirical\": \"a satirical tone with humor and irony.\",\n",
    "            \"conversational\": \"a friendly and informal conversational tone.\",\n",
    "            \"poetic\": \"a poetic style, using metaphorical and rhythmic language.\",\n",
    "            \"investigative\": \"an investigative tone, presenting facts systematically and focusing on analysis.\"\n",
    "        }\n",
    "\n",
    "        style_description = style_descriptions.get(style, \"a general style\")\n",
    "\n",
    "        article = f\"\"\"\n",
    "        **Headline**: Breaking News: {topic}\n",
    "\n",
    "        In a remarkable development, {summary}\n",
    "\n",
    "        This article explores the topic of \"{topic}\" with a focus on the following key points:\n",
    "        - {', '.join(keywords)}\n",
    "\n",
    "        Written in {style_description}, this piece dives into the intricacies of {topic}, providing insights and detailed analysis.\n",
    "        \"\"\"\n",
    "        return article.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claim Verification test zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence segmentation and Claim decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rick/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def segment_article_into_sentences(article):\n",
    "    return sent_tokenize(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentences_as_claims(sentences, model=\"gpt-4\", temperature=0.3):\n",
    "    prompt = \"Below is a list of sentences. For each one, answer YES if it is a factual claim that can be verified (true or false), and NO otherwise.\\n\\n\"\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        prompt += f\"{i+1}. {sentence.strip()}\\n\"\n",
    "\n",
    "    prompt += \"\\nAnswer as: 1. YES, 2. NO, ...\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7,\n",
    "            )\n",
    "\n",
    "    #output = response['choices'][0]['message']['content']\n",
    "    output = response.choices[0].message.content.strip()\n",
    "    lines = output.strip().split(\"\\n\")\n",
    "    claim_sentences = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"YES\" in line.upper():\n",
    "            claim_sentences.append(sentences[i])\n",
    "    return claim_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "article=\"The recent advancements in artificial intelligence have been remarkable. OpenAI released GPT-4 in March 2023, marking a significant milestone in language model development. What does this mean for the future of human creativity? Machine learning algorithms now process over 2.5 quintillion bytes of data daily across the internet. How fascinating it would be if we could harness this computational power for solving climate change! The global AI market is projected to reach $1.8 trillion by 2030. Consider the ethical implications of such rapid technological growth. Google's headquarters are located in Mountain View, California, where the company employs approximately 170,000 people worldwide. Imagine a world where artificial intelligence could predict natural disasters with 99% accuracy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Segment\n",
    "sentences = segment_article_into_sentences(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The recent advancements in artificial intelligence have been remarkable.',\n",
       " 'OpenAI released GPT-4 in March 2023, marking a significant milestone in language model development.',\n",
       " 'What does this mean for the future of human creativity?',\n",
       " 'Machine learning algorithms now process over 2.5 quintillion bytes of data daily across the internet.',\n",
       " 'How fascinating it would be if we could harness this computational power for solving climate change!',\n",
       " 'The global AI market is projected to reach $1.8 trillion by 2030.',\n",
       " 'Consider the ethical implications of such rapid technological growth.',\n",
       " \"Google's headquarters are located in Mountain View, California, where the company employs approximately 170,000 people worldwide.\",\n",
       " 'Imagine a world where artificial intelligence could predict natural disasters with 99% accuracy.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Classify which ones are claims\n",
    "claims = classify_sentences_as_claims(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OpenAI released GPT-4 in March 2023, marking a significant milestone in language model development.',\n",
       " 'Machine learning algorithms now process over 2.5 quintillion bytes of data daily across the internet.',\n",
       " 'The global AI market is projected to reach $1.8 trillion by 2030.',\n",
       " \"Google's headquarters are located in Mountain View, California, where the company employs approximately 170,000 people worldwide.\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_claim(claim: str):\n",
    "    prompt = f\"\"\"\n",
    "            You are a fact-checking assistant. Generate 5 yes or no questions to help me answer if the given claim is true or false.\n",
    "\n",
    "Claim: \"{claim}\"\n",
    "\n",
    "Subquestions:\n",
    "1.\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=300,\n",
    "        temperature=0.7,\n",
    "        stop=None\n",
    "    )\n",
    "    questions = response.choices[0].message.content.strip().split(\"\\n\")\n",
    "    return [q.strip(\"0123456789. \").strip() for q in questions if q.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim=claims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "subqs = decompose_claim(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Did OpenAI actually release GPT-4 in March 2023?',\n",
       " 'Is GPT-4 a language model developed by OpenAI?',\n",
       " 'Was the release of GPT-4 considered a significant milestone in language model development?',\n",
       " 'Was there any official announcement or documentation from OpenAI about the release of GPT-4 in March 2023?',\n",
       " 'Was there any significant technological advancement or features introduced in GPT-4 compared to its predecessors?']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Stage Web Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi.google_search import GoogleSearch\n",
    "from readability.readability import Document\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from readability.readability import Document\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_readable_text(url):\n",
    "    try:\n",
    "        session = requests.Session()\n",
    "        headers = {\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/115.0 Safari/537.36\"\n",
    "            )\n",
    "        }\n",
    "        response = session.get(url, headers=headers, timeout=10)\n",
    "\n",
    "        if response.status_code == 403:\n",
    "            print(f\"[403] Forbidden (likely anti-bot): {url}\")\n",
    "            return None\n",
    "        if response.status_code != 200:\n",
    "            print(f\"[{response.status_code}] Cannot access: {url}\")\n",
    "            return None\n",
    "\n",
    "        html = response.text\n",
    "        doc = Document(html)\n",
    "        summary_html = doc.summary()\n",
    "        soup = BeautifulSoup(summary_html, \"html.parser\")\n",
    "        text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        if len(text.split()) < 30:\n",
    "            print(f\"[Skipped] Too short: {url}\")\n",
    "            return None\n",
    "\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] {url} -> {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "def extract_with_newspaper(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        text = article.text\n",
    "        if len(text.split()) < 30:\n",
    "            print(f\"[Skipped] Too short: {url}\")\n",
    "            return None\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] {url} -> {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_readable_text_combined(url):\n",
    "    text = extract_readable_text(url)\n",
    "    if text:\n",
    "        return text\n",
    "    return extract_with_newspaper(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c6e679102928c9c981d382b4adbc7af8bde06aca989e730c2f9f75cab6545e26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def google_custom_search(query, api_key, cx, num_results=5):\n",
    "    search_url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    search_url = \"https://serpapi.com/search\"\n",
    "    params = {\n",
    "        \"key\": api_key,\n",
    "        \"cx\": cx,\n",
    "        \"q\": query,\n",
    "        \"num\": num_results\n",
    "    }\n",
    "\n",
    "    response = requests.get(search_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    results = response.json()\n",
    "\n",
    "    links = []\n",
    "    for item in results.get(\"items\", []):\n",
    "        links.append({\n",
    "            \"title\": item.get(\"title\"),\n",
    "            \"link\": item.get(\"link\"),\n",
    "            \"snippet\": item.get(\"snippet\")\n",
    "        })\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second stage web retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_rerank(query, documents, k1=30, k2=150, top_k=4):\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        words = nltk.word_tokenize(doc)\n",
    "        for i in range(0, len(words), k1 // 2):\n",
    "            chunk = words[i:i+k1]\n",
    "            chunks.append(\" \".join(chunk))\n",
    "\n",
    "    tokenized_chunks = [nltk.word_tokenize(c.lower()) for c in chunks]\n",
    "    bm25 = BM25Okapi(tokenized_chunks)\n",
    "    scores = bm25.get_scores(nltk.word_tokenize(query.lower()))\n",
    "    top_chunks = [chunks[i] for i in sorted(range(len(scores)), key=lambda i: -scores[i])[:top_k]]\n",
    "    return top_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subquestion: Did OpenAI actually release GPT-4 in March 2023?\n",
      "[403] Forbidden (likely anti-bot): https://help.openai.com/en/articles/6825453-chatgpt-release-notes\n",
      "[Error] https://help.openai.com/en/articles/6825453-chatgpt-release-notes -> Article `download()` failed with 403 Client Error: Forbidden for url: https://help.openai.com/en/articles/6825453-chatgpt-release-notes on URL https://help.openai.com/en/articles/6825453-chatgpt-release-notes\n",
      "[403] Forbidden (likely anti-bot): https://openai.com/index/gpt-4-research/\n",
      "[Error] https://openai.com/index/gpt-4-research/ -> Article `download()` failed with 403 Client Error: Forbidden for url: https://openai.com/index/gpt-4-research/ on URL https://openai.com/index/gpt-4-research/\n",
      "\n",
      "Subquestion: Is GPT-4 a language model developed by OpenAI?\n",
      "[403] Forbidden (likely anti-bot): https://platform.openai.com/docs/models\n",
      "[Error] https://platform.openai.com/docs/models -> Article `download()` failed with 403 Client Error: Forbidden for url: https://platform.openai.com/docs/models on URL https://platform.openai.com/docs/models\n",
      "[403] Forbidden (likely anti-bot): https://openai.com/index/gpt-4-research/\n",
      "[Error] https://openai.com/index/gpt-4-research/ -> Article `download()` failed with 403 Client Error: Forbidden for url: https://openai.com/index/gpt-4-research/ on URL https://openai.com/index/gpt-4-research/\n",
      "[403] Forbidden (likely anti-bot): https://openai.com/index/chatgpt/\n",
      "[Error] https://openai.com/index/chatgpt/ -> Article `download()` failed with 403 Client Error: Forbidden for url: https://openai.com/index/chatgpt/ on URL https://openai.com/index/chatgpt/\n",
      "\n",
      "Subquestion: Was the release of GPT-4 considered a significant milestone in language model development?\n",
      "[403] Forbidden (likely anti-bot): https://openai.com/index/gpt-4-research/\n",
      "[Error] https://openai.com/index/gpt-4-research/ -> Article `download()` failed with 403 Client Error: Forbidden for url: https://openai.com/index/gpt-4-research/ on URL https://openai.com/index/gpt-4-research/\n",
      "[403] Forbidden (likely anti-bot): https://www.sciencedirect.com/science/article/pii/S0268401223000233\n",
      "[Error] https://www.sciencedirect.com/science/article/pii/S0268401223000233 -> Article `download()` failed with 403 Client Error: Forbidden for url: https://www.sciencedirect.com/science/article/pii/S0268401223000233 on URL https://www.sciencedirect.com/science/article/pii/S0268401223000233\n",
      "\n",
      "Subquestion: Was there any official announcement or documentation from OpenAI about the release of GPT-4 in March 2023?\n",
      "[403] Forbidden (likely anti-bot): https://help.openai.com/en/articles/6825453-chatgpt-release-notes\n",
      "[Error] https://help.openai.com/en/articles/6825453-chatgpt-release-notes -> Article `download()` failed with 403 Client Error: Forbidden for url: https://help.openai.com/en/articles/6825453-chatgpt-release-notes on URL https://help.openai.com/en/articles/6825453-chatgpt-release-notes\n",
      "[403] Forbidden (likely anti-bot): https://openai.com/index/gpt-4-research/\n",
      "[Error] https://openai.com/index/gpt-4-research/ -> Article `download()` failed with 403 Client Error: Forbidden for url: https://openai.com/index/gpt-4-research/ on URL https://openai.com/index/gpt-4-research/\n",
      "[403] Forbidden (likely anti-bot): https://platform.openai.com/docs/deprecations\n",
      "[Error] https://platform.openai.com/docs/deprecations -> Article `download()` failed with 403 Client Error: Forbidden for url: https://platform.openai.com/docs/deprecations on URL https://platform.openai.com/docs/deprecations\n",
      "\n",
      "Subquestion: Was there any significant technological advancement or features introduced in GPT-4 compared to its predecessors?\n",
      "[403] Forbidden (likely anti-bot): https://www.sciencedirect.com/science/article/pii/S266734522300024X\n",
      "[Error] https://www.sciencedirect.com/science/article/pii/S266734522300024X -> Article `download()` failed with 403 Client Error: Forbidden for url: https://www.sciencedirect.com/science/article/pii/S266734522300024X on URL https://www.sciencedirect.com/science/article/pii/S266734522300024X\n",
      "[403] Forbidden (likely anti-bot): https://www.sciencedirect.com/science/article/pii/S2950162824000316\n",
      "[Error] https://www.sciencedirect.com/science/article/pii/S2950162824000316 -> Article `download()` failed with 403 Client Error: Forbidden for url: https://www.sciencedirect.com/science/article/pii/S2950162824000316 on URL https://www.sciencedirect.com/science/article/pii/S2950162824000316\n",
      "# documents: 13\n",
      "[Doc 0] 86 words\n",
      "[Doc 1] 625 words\n",
      "[Doc 2] 363 words\n",
      "[Doc 3] 563 words\n",
      "[Doc 4] 666 words\n",
      "[Doc 5] 2343 words\n",
      "[Doc 6] 1825 words\n",
      "[Doc 7] 86 words\n",
      "[Doc 8] 7971 words\n",
      "[Doc 9] 1088 words\n",
      "[Doc 10] 2270 words\n",
      "[Doc 11] 86 words\n",
      "[Doc 12] 1427 words\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "api_key = \"AIzaSyBQNt7wJLePi6oWGGRe07i0oG0VkF_10lc\"\n",
    "cx = \"c58bf6758fa324380\"\n",
    "for subq in subqs:\n",
    "    urls = google_custom_search(subq, api_key, cx)\n",
    "    #print(f\"\\nSubquestion: {subq}\")\n",
    "    for result in google_custom_search(subq, api_key, cx):\n",
    "        url = result[\"link\"]\n",
    "        text = extract_readable_text_combined(url)\n",
    "        if text:\n",
    "            documents.append(text)\n",
    "\n",
    "print(f\"# documents: {len(documents)}\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"[Doc {i}] {len(doc.split())} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_chunks = []\n",
    "for subq in subqs:\n",
    "    top_chunks.extend(bm25_rerank(subq, documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reading : How to Write ChatGPT Prompts : Your Guide Generative AI grows 2023 was a milestone year in terms of generative AI . Not only did OpenAI release GPT-4',\n",
       " 'direct access to the internet for retrieving real-time information or updates . 5 . How Did GPT-4 Address the Challenges of GPT-3 ? GPT-4 : Introduced in 2023 , GPT-4',\n",
       " 'Did GPT-4 Address the Challenges of GPT-3 ? GPT-4 : Introduced in 2023 , GPT-4 aimed to refine and enhance the capabilities of GPT-3 . It focused on improving accuracy',\n",
       " 'a milestone year in terms of generative AI . Not only did OpenAI release GPT-4 , which again built on its predecessorâ\\x80\\x99s power , but Microsoft integrated ChatGPT into its',\n",
       " 'ChatGPT , based on the GPT-4 architecture , which is a language model developed by OpenAI . These models learn patterns in human language and generate responses based on those',\n",
       " 'language model developed by OpenAI , following GPT-4 , GPT-3.5 , and GPT-3 releases . OpenAI boasts that GPT-4o is 50 % cheaper to operate than GPT-4 while being twice',\n",
       " '13 , 2024 Number of Parameters : Unknown GPT-4o is the latest and most advanced language model developed by OpenAI , following GPT-4 , GPT-3.5 , and GPT-3 releases .',\n",
       " 'Azure OpenAI is now in public preview . GPT-4 Turbo with Vision is a large multimodal model ( LMM ) developed by OpenAI that can analyze images and provide textual',\n",
       " 'demonstrating the ability to understand and generate human-like text with remarkable accuracy and coherence . The release of ChatGPT marked a significant milestone in the AI landscape , igniting a',\n",
       " 'The release of ChatGPT marked a significant milestone in the AI landscape , igniting a rapid adoption of generative AI technologies across various industries . This breakthrough has driven innovation',\n",
       " 'features , marking a significant milestone in the evolution of AI language models . As we look to the future , the continuous advancements in AI promise to drive further',\n",
       " 'reading : How to Write ChatGPT Prompts : Your Guide Generative AI grows 2023 was a milestone year in terms of generative AI . Not only did OpenAI release GPT-4',\n",
       " 'any time . To learn more check out the how-to-article . March 2023 GPT-4 series models are now available in preview on Azure OpenAI . To request access , existing',\n",
       " 'deployment ) . The underlying customized model will remain available and can be redeployed at any time . To learn more check out the how-to-article . March 2023 GPT-4 series',\n",
       " 'and makes estimating fine-tuning costs much easier . To learn more , you can consult the official announcement . GPT-4o released in new regions GPT-4o is now also available in',\n",
       " 'the official announcement . GPT-4o released in new regions GPT-4o is now also available in : Sweden Central for standard regional deployment . Australia East , Canada East , Japan',\n",
       " '( ChatGPT . ) What ’ s new in GPT-4 GPT-4 has a number of unique and enhanced features compared to its predecessors . These include : Context retention capability',\n",
       " 'unique and enhanced features compared to its predecessors . These include : Context retention capability The GPT-4 model has the ability to retain the context of the conversation and use',\n",
       " 'GPT-4o represents the latest advancement in OpenAI ’ s language models , building upon the strengths of its predecessors while introducing several groundbreaking features . Key Enhancements in GPT-4o :',\n",
       " 'direct access to the internet for retrieving real-time information or updates . 5 . How Did GPT-4 Address the Challenges of GPT-3 ? GPT-4 : Introduced in 2023 , GPT-4']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_context = \"\\n\\n\".join(top_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claim Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_claim_focused(claim: str, evidence: str):\n",
    "    prompt = f\"\"\"\n",
    "Suppose you are assisting a fact-checker to fact-check the claim.\n",
    "\n",
    "Claim: \"{claim}\"\n",
    "\n",
    "Document:\n",
    "\\\"\\\"\\\"\n",
    "{evidence}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Summarize the relevant information from the document in 1-2\n",
    "sentences. Your response should provide a clear and concise\n",
    "summary of the relevant information contained in the document.\n",
    "Do not include a judgment about the claim and do not repeat any\n",
    "information from the claim that is not supported by the\n",
    "document.\n",
    "Summarization:\n",
    "\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=300,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = summarize_claim_focused(claim, evidence_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The document confirms that OpenAI released GPT-4 in the year 2023. However, it does not specify the month of the release as March 2023.'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt4_classify_veracity(claim, evidence_summary):\n",
    "    prompt = f\"\"\"\n",
    "You are a professional fact-checking assistant. Your task is to read a factual claim and a summary of supporting or opposing evidence, and classify the claim's truthfulness into one of the following 6 categories:\n",
    "\n",
    "- true\n",
    "- mostly true\n",
    "- half true\n",
    "- barely true\n",
    "- false\n",
    "- pants-on-fire\n",
    "\n",
    "### Guidelines:\n",
    "- \"true\": The evidence clearly confirms all factual aspects of the claim.\n",
    "- \"mostly true\": The evidence confirms most aspects, with minor issues or missing context.\n",
    "- \"half true\": The evidence is mixed, with significant confirmations and contradictions.\n",
    "- \"barely true\": Only a small part of the claim is supported by evidence.\n",
    "- \"false\": The claim is directly contradicted by the evidence.\n",
    "- \"pants-on-fire\": The claim is not only false but wildly inaccurate or fabricated.\n",
    "\n",
    "### Examples:\n",
    "\n",
    "**Claim**: The U.S. has the highest number of gun deaths in the world.\n",
    "**Evidence**: The U.S. ranks 32nd in gun deaths per capita globally. Countries like El Salvador, Venezuela, and Honduras have much higher rates. However, the U.S. does lead in total number of gun deaths due to population size.\n",
    "**Classification**: half true\n",
    "\n",
    "**Claim**: The moon landing was faked and filmed in a studio.\n",
    "**Evidence**: There is overwhelming scientific, photographic, and third-party evidence confirming the Apollo 11 landing in 1969. The conspiracy theory lacks credible support.\n",
    "**Classification**: pants-on-fire\n",
    "\n",
    "---\n",
    "\n",
    "Now evaluate this case:\n",
    "\n",
    "**Claim**: {claim}\n",
    "\n",
    "**Evidence**:\n",
    "{evidence_summary}\n",
    "\n",
    "**Classification**:\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=20\n",
    "    )\n",
    "\n",
    "    label = response.choices[0].message.content.strip().lower()\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = gpt4_classify_veracity(claim, final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mostly true'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
